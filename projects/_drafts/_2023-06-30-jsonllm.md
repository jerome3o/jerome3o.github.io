---
layout: post
title: "JsonLLM: Getting Open Source LLMs to follow some rules"
---

Over the last few months I've been making putting a lot of time into learning about machine learning, LLMs in particular, for economic and personal reasons. I've become extremely interested in getting a decent language model running on prem as a home assistant of sorts, automating complicated tasks while keeping all of my data on prem. I think if AI is going to be running a large part of my life I'd like to know how it all works, and be running the majority of it myself. 

When trying to use the Vicuna 13B model from lmsys (and a few other open source ones) to run agents, I quickly ran into issues parsing language model outputs. 

For example I wanted an agent to run when I received an email, determine if it should add an event to my calendar and if so, produce the necessary inputs for an `add_event_to_calendar` function.

The function is written in python and requires 3 arguments: title, start time, end time. Getting the language model to output data in a parseable format reliably proved difficult, it would often output invalid date formats and produce json objects that didn't match the required format.

* Been learning about LLMs, trying to make some agents
* I want it to be all on prem, self hosted, all my data stays on-site
* Open source LLMs are decent for many things
    * But ensuring that it produces valid arguments for a python function is hard
* Little calendar example?
* Description of how LLMs work, token generation etc
* Prior art of token masking (ReLLM, ParserLLM, jsonformer)
* Discussion of jsonllm
    * Converts jsonschema into cfg
    * Uses lark to make a parser
    * Hands that over to ParserLLM
    * Which breaks it down into regular expressions and leverages ReLLM.
* Link to code and some examples (with a nice pydantic code snippet)
