---
layout: post
title: "JsonLLM: Getting Open Source LLMs to follow some rules"
---

Over the last few months I've been making putting a lot of time into learning about machine learning, LLMs in particular, for economic and personal reasons. I've become extremely interested in getting a decent language model running on prem as a home assistant of sorts, automating complicated tasks while keeping all of my data on prem. I think if AI is going to be running a large part of my life I'd like to know how it all works, and be running the majority of it myself. 

When trying to use the Vicuna 13B model from lmsys (and a few other open source ones) to run agents, I quickly ran into issues parsing language model outputs. 

For example I wanted an agent to run when I received an email, determine if it should add an event to my calendar and if so, produce the necessary inputs for an `add_event_to_calendar` function.

The function is written in python and requires 3 arguments: title, start time, end time. Getting the language model to output data in a parseable format reliably proved difficult, it would often output invalid date formats and produce json objects that didn't match the required format.

There are many ways to tackle this problem, here are some potential solutions
* Use a better LLM, GPT-4 produces very reliable outputs
* Use prompt engineering and make some smart chained LLM calls
* Prune tokens at generation time so that only tokens adhering to your specification are selected 

Seeing as I wanted to do everything on prem, and don't want to buy an 8xA100 rig for my little home server, using bigger models was not a viable option. 

The prompt engineering route is likely to have varying reliability between models, as some prompt engineering tricks may differ in efficacy between LLMs. 

The third option is a bit more complicated but would provide the reliability I was after. And depending on how it was implemented could be very versatile. 

And because it would be able to guarantee a given structure, I could make it produce JSON objects that follow a provided jsonschema. That alongside pydantics ability to provide jsonschema for a model, I should be able to create a very pythonic/pydantic interface to the text generation. For example, following the calendar theme from before I should be able to do something like:

```python
model = load_llm()

class CalendarEvent(BaseModel):
    start_time: datetime
    end_time: datetime
    title: str

result = generate_json_from_schema(
    prompt="define a calendar event in json starting at... etc",
    schema=CalendarEvent.schema(),
    model=model, 
)

calendar_event = CalendarEvent(**result)
```

Or similar, in this case `generate_json_from_schema` is a function returning a parsed json object from the guided model generation.

Before jumping into the details let's take a quick overview of how LLMs generate text

# LLM Text Generation

* summary of structure
* NN evaluation overview, ending with token prob dist
* top k, top p, sampling, beam, etc
* discussion of how json LLM prunes tokens

# Prior Art

* Jsonformer
* ReLLM
* ParserLLM
* OpenAI functions (not sure how it works)


# JsonLLM Implementation

JsonLLM leverages ParserLLM heavily, it takes in a JsonSchema, converts it to a context free grammar, and then hands the generation over to ParserLLM.

This isolated the complexity to a translation between two schemas. I'm unsure if I'll be able to represent all the jsonschema features in context free grammars, but so far I've been able to quickly implement enough to be useful (objects, lists, strings, numbers, booleans). And adding new features should be relatively straightforward, by just adding to/adjusting the jsonschema to CFG script.

# Misc Learnings

* partial regex matching in the regex library
* finding appropriate stopping criteria for regex matching
* logit processor in transformers 
* speed up by not generating constant regex values

## Blog post structure/brainstorm

* Been learning about LLMs, trying to make some agents
* I want it to be all on prem, self hosted, all my data stays on-site
* Open source LLMs are decent for many things
    * But ensuring that it produces valid arguments for a python function is hard
* Little calendar example?
* Description of how LLMs work, token generation etc
* Prior art of token masking (ReLLM, ParserLLM, jsonformer)
* Discussion of jsonllm
    * Converts jsonschema into cfg
    * Uses lark to make a parser
    * Hands that over to ParserLLM
    * Which breaks it down into regular expressions and leverages ReLLM.
* Link to code and some examples (with a nice pydantic code snippet)
