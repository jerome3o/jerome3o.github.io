---
layout: post
title: "JsonLLM: Getting Open Source LLMs to follow some rules"
---

## The Motivation

I currently work in Aerospace, specifically in rocket launch services. This industry is [heavily regulated](https://cloud.google.com/security/compliance/itar) and a lot of knowledge is strictly export controlled, which means using a tool like GitHub Copilot is out of the question.

Having experienced the AI productivity boost at home, I desperately tried to get an open source on-prem Copilot-like service like [FauxPilot](https://github.com/fauxpilot/fauxpilot) or [Tabby](https://www.tabbyml.com/) running on-site. This worked with varying degrees of success, however with the compute available I wasn't able to get anything nearly as good as Copilot.

I did however end up getting my hands dirty with some open source LLMs - and became particularly interested in running private on-prem agents, like a real sci-fi AI personal assistant. I wanted something that could read my emails, and edit my calendar, and maybe in the future plan whole events and buy things on my behalf. I wanted it to be all on prem, self hosted, so that all my data stays on-site. Also I think if AI is going to be running a large part of my life I'd like to know how it all works, and be running the majority of it myself.

## The Problem

Open source models have approximate knowledge of many things, and are *reasonably* smart when pushed in the right directions. I found they can get the jist of what I wanted, but when you actually get down to trying to produce structured parsable text to build function arguments from, it was particularly flakey.

For example, I want it to be able to read an email, and put events in my calendar. A naive way to do this would be to write a python script that scans for new emails, and on each email prompt the AI with something like (note this is using completion, not chat):

```python
prompt = """\
Content of an email:
{email_content}

List of calendar events in JSON format like \{"title": "title_string", "date": "dd-mm-yyyy"\}:
""".format(email_content=email_content)
```

And then have some smart python code to parse the JSON, validate the values, and pass them to a python function that calls the Gmail API. This is prone to many formatting, parsing, and validation issues, like it might get the date wrong, hallucinate extra fields, or not even return the desired json at all.

GPT-4 could probably do it easily, but the open source models I can run (like Vicuna 13B) would have a hard time doing it reliably. Also as you start doing more complicated stuff, even GPT-4 struggles.

The fundamental idea here is that you are relying on the LLM to understand the *format* you want the data in (JSON), as well as the idea you want it to produce (new calendar events). The former is very strict, grammatical, and can be defined programmatically, and the latter is purely semantic in the realm of natural language understanding, which is the magic that these LLMs are useful for.

## The Solution

There are many ways to tackle the parsing problem, here are some potential solutions:

* Use a better LLM, (i.e. use GPT-4)
* Use prompt engineering and make some smart chained LLM calls (i.e. [TypeChat](https://github.com/microsoft/TypeChat)/[Guidance](https://github.com/microsoft/guidance))
* Prune tokens at generation time so that only tokens adhering to your specification are selected (more explanation to come)

Seeing as I wanted to do everything on premise, and don't want to buy an 8xA100 rig for my little home server, using bigger models was not a viable option.

The prompt engineering route is likely to have varying reliability between models, as some prompt engineering tricks may differ in efficacy between LLMs. It also may be extremely inefficient, and you're still relying on the model to understand the format.

The third option is a bit more complicated but would guarantee the generation would be valid according to some specification. And depending on how it was implemented could be very versatile. This would mean I could have an LLM produce JSON objects that follow a provided [JsonSchema](https://json-schema.org/).

Before jumping into the details let's take a quick overview of how LLMs generate text

# The Implementation

TODO get image from lucidchart

## Prior Art

* [Jsonformer](https://github.com/1rgs/jsonformer)
    * Implements a state machine for JsonSchema, prunes tokens according to the state machine
* [ReLLM](https://github.com/r2d4/rellm)
    * Uses partial matching of regex to prune tokens so that the generated string matches the pattern.
* [ParserLLM](https://github.com/r2d4/parserllm)
    * Uses CFGs and the Lark library to get the regex for use in ReLLM, resulting is generations that match the CFG
* [OpenAI functions](https://openai.com/blog/function-calling-and-other-api-updates)
    * Unknown, looks like prompt engineering or similar, but there is logit bias in some of their other APIs

Things that popped up while writing this post:
* [LMQL](https://lmql.ai/)
* [Automorphic's Trex](https://automorphic.ai/)
* [gpt-json](https://github.com/piercefreeman/gpt-json)
* [llama.ccp adding support](https://github.com/ggerganov/llama.cpp/pull/1773#issue-1749135073), and [here](https://github.com/ggerganov/llama.cpp/pull/1397)

## JsonLLM Implementation

JsonLLM leverages ParserLLM heavily, it takes in a JsonSchema, converts it to a context free grammar, and then hands the generation over to ParserLLM.

This isolated the complexity to a translation between two schemas. I'm unsure if I'll be able to represent all the jsonschema features in context free grammars, but so far I've been able to quickly implement enough to be useful (objects, lists, strings, numbers, booleans). And adding new features should be relatively straightforward, by just adding to/adjusting the jsonschema to CFG script.


# Misc Learnings

* partial regex matching in the regex library
* finding appropriate stopping criteria for regex matching
* logit processor in transformers
* speed up by not generating constant regex values
* use the summed probability to choose when to terminate potentially infinite sequences


# Pydantic Synergy

That alongside pydantic's ability to provide JsonSchema for a model, I should be able to create a very pythonic/pydantic interface to the text generation. For example, following the calendar theme from before I should be able to do something like:

```python
model = load_llm()

class CalendarEvent(BaseModel):
    start_time: datetime
    end_time: datetime
    title: str

result = generate_json_from_schema(
    prompt="define a calendar event in json starting at... etc",
    schema=CalendarEvent.schema(),
    model=model,
)

calendar_event = CalendarEvent(**result)
```

Or similar, in this case `generate_json_from_schema` is a function returning a parsed json object from the guided model generation.
