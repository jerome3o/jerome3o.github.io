---
layout: post
title: "JsonLLM: Getting Open Source LLMs to follow some rules"
---

## The Motivation

I currently work in Aerospace, specifically in rocket launch services. This industry is [heavily regulated](https://cloud.google.com/security/compliance/itar) and a lot of knowledge is strictly export controlled, which means using a tool like GitHub Copilot is out of the question.

Having experienced the AI productivity boost at home, I desperately tried to get an open source on-prem Copilot-like service like [FauxPilot](https://github.com/fauxpilot/fauxpilot) or [Tabby](https://www.tabbyml.com/) running on-site. This worked with varying degrees of success, however with the compute available I wasn't able to get anything nearly as good as Copilot.

I did however end up getting my hands dirty with some open source LLMs - and became particularly interested in running private on-prem agents, like a real sci-fi AI personal assistant. I wanted something that could read my emails, and edit my calendar, and maybe in the future plan whole events and buy things on my behalf. I wanted it to be all on prem, self hosted, so that all my data stays on-site. Also I think if AI is going to be running a large part of my life I'd like to know how it all works, and be running the majority of it myself.

## The Problem

Open source models have approximate knowledge of many things, and are *reasonably* smart when pushed in the right directions. I found they can get the jist of what I wanted, but when you actually get down to trying to produce structured parsable text to build function arguments from, it was particularly flakey.

For example, I want it to be able to read an email, and put events in my calendar. A naive way to do this would be to write a python script that scans for new emails, and on each email prompt the AI with something like (note this is using completion, not chat):

```python
prompt = """\
Content of an email:
{email_content}

List of calendar events in JSON format like \{"title": "title_string", "date": "dd-mm-yyyy"\}:
""".format(email_content=email_content)
```

And then have some smart python code to parse the JSON, validate the values, and pass them to a python function that calls the Gmail API. This is prone to many formatting, parsing, and validation issues, like it might get the date wrong, hallucinate extra fields, or not even return the desired json at all.

GPT-4 could probably do it easily, but the open source models I can run (like Vicuna 13B) would have a hard time doing it reliably. Also as you start doing more complicated stuff, even GPT-4 struggles.

The fundamental idea here is that you are relying on the LLM to understand the *format* you want the data in (JSON), as well as the idea you want it to produce (new calendar events). The former is very strict, grammatical, and can be defined programmatically, and the latter is purely semantic in the realm of natural language understanding, which is the magic that these LLMs are useful for.

## The Solution

There are many ways to tackle the parsing problem, here are some potential solutions:

* Use a better LLM that can understand the format (i.e. use GPT-4)
* Use clever prompt engineering and build your structured output from some smart chained LLM calls
* Guide the LLM at generation time to ensure that it always produces valid text (with logit processing)

Seeing as I wanted to do everything on premise, and don't want to buy an 8xA100 rig for my little home server, using bigger models was not a viable option.

The prompt engineering route is likely to have varying reliability between models, as some prompt engineering tricks may differ in efficacy between LLMs. It also may be extremely inefficient, and you're still relying on the model to understand the format.

The third option is a bit more complicated but would guarantee the generation would be valid according to some specification. And depending on how it was implemented could be very versatile. This would mean I could have an LLM produce JSON objects that follow a provided [JsonSchema](https://json-schema.org/).

I ended up going with the third option, logit processing.

### Logit Processing

When a GPT-like language model is generating text, it starts with a user provided prompt, tokenizes the text, embeds the tokens as vectors, and runs that sequence of embedded tokens through a complicated neural network (not going into detail here as it is not relevant). The result from the network is a logit per token in the model's vocabulary, and using a softmax operation each token's logit is converted to the probability that that specific token will come next in the sequence (according to the model). Here is a brief overview:

![diagram](/projects/assets/jsonllm/jsonllm_1.svg)

In the above example the token with the highest probability is chosen, this isn't always the case, and there are many potential ways to sample the next token (see beam search, top k, top p methods).

In essence, the LLM guidance method I want to use will tap into the text generation process at step three and lower the probabilities of (a.k.a. mask) invalid tokens. This means the generated text will always follow a certain pattern, the hard part now being how do we choose what tokens to mask?

### Regular Expressions

When describing patterns in text, regular expressions are king. Leveraging these expressions to guide LLM text generation would be a great start, and to back that up, a bunch of really smart people have already built tools to do it üòç. [ReLLM](https://github.com/r2d4/rellm) is a good example of such works, and is the one I chose to base my work on.

The way this works is buy leveraging "partial matches". This isn't in the python built-in `re` package but a feature of the [`regex`](https://github.com/mrabarnett/mrab-regex#added-partial-matches-hg-issue-102) library, and is basically a match on a string that *could* still end up matching the pattern.

For example consider the pattern `[abc]+[0-9]` which matches any sequence of the a,b,c characters (with at least 1 character) followed by a single digit.

Some standard full matches (meaning they match the whole string) are:
* `aaaaa1`
* `bca2`
* `bbbbbbbb4`

Some partial matches are (any combination of `abc` characters):
* `bbbbbb`
* `abc`

Some examples of strings that won't fully match (they may contain matches, but the whole string isn't a match):
* `1abc`
* `abc5abc`
* `sabc3`

So what we can do is ensure that the language model's whole generation is at least a partial match of the provided pattern. At each token selection step, for each potential token, we can concatenate the token text to the end of the current completion and check if the result is at least a partial match for the provided pattern. If the result doesn't match we can set the probability of that token to 0 (by setting the logit to a massively negative number), and then continue with the standard token sampling. This will result in the generation only being able to select tokens that adhere to the pattern.

### Context Free Grammars

While regular expressions are valuable for describing patterns, they do fall short in certain areas. For instance, they lack the capacity to describe content that follows a specific syntax with nested patterns, which is definitely required for things like JSON data or python function arguments.

But also found in the well established computer science problem-solving toolbox, there is something designed specifically to operate beyond the limitations of regular expressions - and that tool is the Context-Free Grammar (CFG).

Fundamentally, a Context-Free Grammar is a set of rules that describe all possible strings in a given formal language. It acts as a tool that provides guidance on language syntax, useful for programming languages, and provides the basis for parsing in many compiler designs.

For example, this is a rough CFG that describes valid JSON objects:

```
JSON := OBJECT | ARRAY
OBJECT := '{' PAIRLIST? '}'
PAIRLIST := PAIR (',' PAIR)*
PAIR := STRING ':' VALUE
VALUE := STRING | NUMBER | OBJECT | ARRAY | 'true' | 'false' | 'null'
ARRAY := '[' VALUELIST? ']'
VALUELIST := VALUE(',' VALUE)*
STRING := '"' [a-z, A-Z, 0-9]* '"'
NUMBER := [0-9]+
```

By using this CFG to mask invalid tokens at each step of the LLM generation process, much like how we did with the regular expressions, we could ensure that the final output is a valid JSON object. Implementing it, however, is a bit more complicated, but like with regular expressions there is some prior art out there in the open source community! See [ParserLLM](https://github.com/r2d4/parserllm), this is the one I've based my work on, it's made by the same person as ReLLM, [Matt Rickard](https://github.com/r2d4).

TODO: quick description of how ParserLLM works

Awesome! so now we can guide LLM generation (mostly) with Context Free Grammars - now the only hard part is making the CFG. This is a pretty involved task - seeing as most of my usecases are satisfied with JSON objects there is room for some automation here

### Json Schema

Before you say it - this section actually involves some novel contributions from myself (not just borrowing from the open source community).

CFGs are very powerful and very cool, but they are not commonly known or used by the average software developer - I really wanted to make this guided LLM completion easy to use.

I am a big fan of the [pydantic](https://docs.pydantic.dev/latest/) python library and was keen to find a way to use pydantic models to guide LLM generation. Most python classes inheriting from `pydantic.BaseModel` have a `.schema()` method that can be used to generate a [JsonSchema](https://json-schema.org/) for the model. These JsonSchema's are a schematic that completely describes what a valid JSON object looks like for that model.

This is a perfect starting point, if I am able to automatically convert a JsonSchema to a CFG that matches valid JSON objects of that schema, I could leverage pydantic to make an extremely accessible way of guiding LLM generation.

So I have written a very basic JsonSchema to CFG compiler [here](https://github.com/jerome3o/home-llm/blob/main/jsonllm/server/jsonschema2cfg.py) that supports the basic attributes of JsonSchema, and is easy to extend as needed. This is awesome, because it allows me to write some very clean python code to guide LLM generation.

For example, all combined together, code like this is *possible* (still needs to be implemented this cleanly):

```python
# Load in a language model
model = load_llm()

# Define the desired shape of your LLM output, i.e. the attributes you'd like
class CalendarEvent(BaseModel):
    start_time: datetime
    end_time: datetime
    title: str

# Pass the model schema, and the prompt into our LLM generation framework
event = generate_json_from_schema(
    prompt="define a calendar event in json starting at... etc",
    schema=CalendarEvent.schema(),
    model=model,
)

# Call your python function to add the event to your calendar
add_calendar_event(event, calendar_service)
```

## The Implementation

## Prior Art

* [Jsonformer](https://github.com/1rgs/jsonformer)
    * Implements a state machine for JsonSchema, prunes tokens according to the state machine
* [ReLLM](https://github.com/r2d4/rellm)
    * Uses partial matching of regex to prune tokens so that the generated string matches the pattern.
* [ParserLLM](https://github.com/r2d4/parserllm)
    * Uses CFGs and the Lark library to get the regex for use in ReLLM, resulting is generations that match the CFG
* [OpenAI functions](https://openai.com/blog/function-calling-and-other-api-updates)
    * Unknown, looks like prompt engineering or similar, but there is logit bias in some of their other APIs

Things that popped up while writing this post:
* [LMQL](https://lmql.ai/)
* [Automorphic's Trex](https://automorphic.ai/)
* [gpt-json](https://github.com/piercefreeman/gpt-json)
* [llama.ccp adding support](https://github.com/ggerganov/llama.cpp/pull/1773#issue-1749135073), and [here](https://github.com/ggerganov/llama.cpp/pull/1397)

## JsonLLM Implementation

JsonLLM leverages ParserLLM heavily, it takes in a JsonSchema, converts it to a context free grammar, and then hands the generation over to ParserLLM.

This isolated the complexity to a translation between two schemas. I'm unsure if I'll be able to represent all the jsonschema features in context free grammars, but so far I've been able to quickly implement enough to be useful (objects, lists, strings, numbers, booleans). And adding new features should be relatively straightforward, by just adding to/adjusting the jsonschema to CFG script.


# Misc Learnings

* partial regex matching in the regex library
* finding appropriate stopping criteria for regex matching
* logit processor in transformers
* speed up by not generating constant regex values
* use the summed probability to choose when to terminate potentially infinite sequences


# Pydantic Synergy

That alongside pydantic's ability to provide JsonSchema for a model, I should be able to create a very pythonic/pydantic interface to the text generation. For example, following the calendar theme from before I should be able to do something like:

```python
model = load_llm()

class CalendarEvent(BaseModel):
    start_time: datetime
    end_time: datetime
    title: str

result = generate_json_from_schema(
    prompt="define a calendar event in json starting at... etc",
    schema=CalendarEvent.schema(),
    model=model,
)

calendar_event = CalendarEvent(**result)
```

Or similar, in this case `generate_json_from_schema` is a function returning a parsed json object from the guided model generation.
